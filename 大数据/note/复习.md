# 数据挖掘是什么

有效性，可用性，出乎意料，可解释

# Hadoop生态圈

组件，功能

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec2/4.png)

| 组件        | 描述                                                                                                                     |
| --------- | ---------------------------------------------------------------------------------------------------------------------- |
| ZooKeeper | 调度管理组件，分布式应用程序协调服务                                                                                                     |
| Oozie     | 工作流和定时器，对MapReduce和Pig Jobs任务调度协调                                                                                      |
| Pig       | 对MapReduce进行抽象，可以理解为接口，被其他生态成员调用                                                                                       |
| Hive      | 类型于SQL的高级语言，直接在Hadoop上进行查询                                                                                             |
| Sqoop     | 迁移工具，主要进行迁移，数据集成                                                                                                       |
| Flume     | Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方(可定制)的能力。 |
| Mahout    | 提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序                                                                          |

mapReduce原理，步骤

- MapReduce计算模型非常适合在大量计算机组成的大规模集群上并行运行。每一个map任务和每一个reduce任务均可以同时运行于一个单独的计算节点上，可想而知，其运算效率是很高的。
- 并行计算过程：
  1. 数据分布存储
  2. 分布式并行计算
  3. 本地计算:是一种减少带宽消耗的方法
  4. 任务粒度:粒度下降，大的数据切分成小的数据，一个单位的数据尽量小于一个Block的大小，在一个节点上。
  5. 数据分割(Partition)
  6. 数据合并(Combine):可以理解成为是和Reducer一致的东西
  7. Reduce
  8. 任务管道

# 三种模式集群

本地 后台应用 本机

## 单机模式（独立模式）（Local或Standalone  Mode）

默认情况下，Hadoop即处于该模式，用于开发和调试。

## 伪分布式模式（Pseudo-Distrubuted Mode）

Hadoop的守护进程运行在本机机器，模拟一个小规模的集群在一台主机模拟多主机。

## 全分布式集群模式（Full-Distributed Mode）

Hadoop的守护进程运行在一个集群上，Hadoop的守护进程运行在由多台主机搭建的集群上，是真正的生产环境。

# HDFS架构

1. 使用了主从数据库
2. Client是用户访问数据的接口
3. Block：将一个文件进行分块，通常是64M。写入后不能修改，但是可以追加。
4. NameNode：保存整个文件系统的目录信息、文件信息及分块信息，如果主 NameNode 失效，切换到Secondary NameNode。
5. DataNode：分布在廉价的计算机上，用于存储Block块文件。

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec2/7.png)

# HBase

- HBase是一个分布式、非结构化、稀疏、面向列的数据库。效率比Hive高
- HBase是基于HDFS，山寨版的BigTable，继承了可靠性、高性能、可伸缩性
- 不支持SQL

# Hive

支持SQL

可以将SQL语句转换为MapReduce任务进行运行。延时较高

# PageRank

转移概率

# SVD

# 朴素贝叶斯

## 3.1. 条件概率

1. 指在事件B发生的情况下，事件A发生的概率，用P(A|B)表示，读作在B条件下的A的概率

## 3.2. 全概率公式

P(A) = P(A|B_1) + P(A|B_2) + ... + P(A|B_N)

## 3.3. 贝叶斯公式

1. 后验概率 = 先验概率 * 调整因子
2. P(A|B)=P(A)*\frac{P(B|A)}{P(B)}

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/4.png)

## 3.4. 贝叶斯的例子

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/5.png) ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/6.png)

## 3.5. 朴素贝叶斯的概率模型

1. 一个待分类项:X = f_1, f_2, f_3,..., f_n
2. 类别集合:C_1, C_2, ...,C_m
3. 需要计算:P(C_1|X), P(C_2|X), ...,P(C_m|X)
4. P(f_1|C_1), P(f_2|C_1),..., P(f_n|C_1), P(f_1|C_2), ...,P(f1|C_m),...,P(f_n|C_m)
5. 如果P(C_k|X) = max(P(C_1|X), P(C_2|X), ...,P(C_m|X)),则X \in C_k, P(C_i|X) = \frac{P(X|C_i) * P(C_i)}{P(X)}
6. P(X|C_i)P(C_i) = P(f_1|C_i)*P(f_2|C_i),...,P(f_n|C_i)P(C_i) = P(C_i)\prod\limits_{j=1}\limits^nP(f_i|C_i)

# 决策树之ID3算法

### 4.2.1. 信息熵

1. 对于有k个类别的分类问题，假定样本集合D中第k类样本所占的比例是p_k(k=1, 2, 3, ...,k)，则样本集合D的信息熵为:

$$
Ent(D) = -\sum\limits^k\limits_{k=1}p_k*\log_2 p_k
$$

2. 是刻画系统整体的混乱程度的量，信息熵越大，系统越混乱。

### 4.2.2. 信息增益

> 在决策树的分类问题中，信息增益是针对一个特征T，计算原有数据的信息熵与引用特征后的信息熵之差，信息增益的定义为:

$$
Gain(D, a) = Ent(D) - \sum\limits^V\limits_{v=1} \frac{|D^k|}{|D|} * Ent(D^v)
$$

### 4.2.3. 信息熵和信息增益的计算示例

> 根节点的信息熵

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/12.png) ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/11.png)

> 部分特征的信息增益

| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/13.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/14.png) |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/15.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/16.png) |
| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/17.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/18.png) |

> 最终计算结果:使用信息增益最大的特征作为本节点，其取值作为其他几个子节点

| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/19.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/20.png) |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |

### 4.2.4. ID3算法过程描述

1. 计算给定样本分类所需的信息熵
2. 计算每个特征的信息熵、信息增益
3. 从所有特征列中选出信息增益最大的特征作为根节点或者内部结点-划分结点，其子节点分别为该特征的可能取值
4. 根据划分结点的不同取值来查分数据集为对应子节点，然后删去当前的特征列，在计算剩余列的信息熵，如果有信息增益就重复第二步直到划分结束
5. 划分结束的标志:子集只有一个类别标签，停止划分。

### 4.2.5. ID3算法的不足

1. ID3没考虑**连续特征**，比如长度、密度都是连续值，无法在ID3运用。
2. **ID3用信息增益作为标准容易偏向取值较多的特征**然而在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值比取2个值的信息增益大。如何校正这个问题？
3. ID3算法**没考虑缺失值**问题。
4. **没考虑过拟合**问题。

# C4.5算法

1. C4.5算法是ID3算法的改进，C4.5克服了ID3的两个缺点
   1. 不能处理连续特征
   2. 用信息增益选择属性时偏向于选择分支较多的特征值，即取值多的特征
2. [C4.5算法实例](https://blog.csdn.net/zjsghww/article/details/51638126)

### 4.3.1. 将连续的特征离散化

1. 将n个连续的样本按值从小到大排列，得到数据a_1, a_2, ..., a_n
2. 取相邻两样本值的平均数，会得到n-1个划分点，其中第i个划分点T_i = \frac{a_i + a_{i+1}}{2}
3. 对这n-1个点，分别计算每一个点作为二元分类点时的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点
4. 用信息增益比选择最佳划分
5. 注意这里还不使用信息增益率

| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/21.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/22.png) |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |

### 4.3.2. 信息增益率

> 信息增益率是信息增益与特征熵的比，特征数越多的特征对应的特征熵越大。

$$
\begin{array}{l}
   Gain(D,a) = Ent(D) - \sum\limits^k\limits_{v=1}Ent(D^v) \\
   Gain_{ratio}(D,a) = \frac{Gain(D,a)}{Iv(a)} \\
   IV(a) = -\sum\limits^V\limits_{v=1}\frac{|D^v|}{|D|}\log_2\frac{|D^V|}{|D|}
\end{array}
$$

### 4.3.3. 对于缺失值处理的问题

1. 主要解决两个问题
   1. 样本某些特征缺失的情况下选择划分的属性
   2. 选定了划分属性，对于在该属性上缺失特征的样本的处理
2. 第一个问题:C4.5的想法是将数据分为两部分，每一个部分都设置权重(初始为1)，然后划分数据，一部分有特征A的D1，一部分没有特征A的数据D2，然后对于没有确实特征A的数据集D1来和对应A特征的各个特征值一起计算加权重后的信息增益比，最后乘以一个系数(系数=无特征A缺失的样本加权后所占加权总样本的比例)
3. 第二个问题:可以将缺失特征的样本同时划分入所有的子节点，不过该样本的权重按照各个子样本的数量比例来分类，比如缺失特征A的样本a之前权重为1，特征有3个特征值A1，A2，A3，分别对应的无A特征的样本个数为2，3，4，则a同时划入A1，A2，A3，对应权重调节为2/9、3/9、4/9

### 4.3.4. C4.5算法的不足与改进

1. 决策树算法很容易过拟合，所以决策树需要剪枝，而C4.5的剪枝方法有优化空间
   1. 预剪枝:生成决策树的时候决定是否剪枝
   2. 后剪枝:生成决策树，再通过交叉检验进行剪枝
2. 生成多叉树，不如二叉树计算快
3. 只能用于分类
4. 使用了熵模型，对于连续属性还有大量排序，如果为了检查模型并不希望牺牲太多准确性，使用基尼系数代替熵模型。

# KNN算法

- KNN算法又被称为最近邻算法
- 算法思想:一个样本与数据集中的k个样本最相似，如果这k个样本中大多数属于一个类别，那么这个样本也属于这个类别

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec6/28.png)

## 5.1. 距离度量

1. 选择两个实例相似性时，一般使用的是欧氏距离，Lp距离定义L_p(x_i, x_j) = (\sum\limits_{l=1}\limits^n|x_{i}^{(l)} - x_j^{(l)}|^p)^{\frac{1}{p}}
2. 其中x_i\in R^n, x_j \in R^n，其中L_{\infty}定义为L\infty(x_i, x_j) = max_{l}|x_i^{(l)}-x_j^{(l)}|，其中p是一个变参数
   1. p=1，就是曼哈顿距离(对应L1范数)
   2. p=2，就是欧式距离(对应L2范数)
   3. p \to \infty，就是切比雪夫距离

$$
L_1定义为|x|=\sum\limits_{i=1}\limits^n|x_i|，其中x=\begin{bmatrix}
   x_1 \\ x_2 \\ . \\ . \\ . \\ x_n
\end{bmatrix} \in R^n
$$$$
L_2定义为|x|=\sqrt{\sum\limits_{i=1}\limits^{n}x_i^2}，其中x=\begin{bmatrix}
   x_1 \\ x_2 \\ . \\ . \\ . \\ x_n
\end{bmatrix} \in R^n
$$$$
\begin{array}{l}
   n维空间点a(x_{11},x_{12},...x_{1n})与b(x_{21}, x_{22}, ..., x_{2n}) \\
   d_{12} = \max(|x_{1i}-x_{2i}|)
\end{array}
$$

## 5.2. K值的选择

1. 近似误差:对现有训练集的训练误差
2. 估计误差:对测试集的测试误差，估计误差小，说明对未知数据的预测能力好
3. K值较小，则是在较小的邻域中的训练实例进行预测，容易导致过拟合。
   1. 学习的近似误差会减小:只有输入实例较近的训练实例才会对预测结果起作用
   2. 学习的估计误差会增大:预测结果会对紧邻的实例点敏感，但是如果是噪声会导致预测出错
4. K值较大，则是在较大的邻域中的训练实例进行预测
   1. 学习的估计误差会减小
   2. 学习的近似误差会增大
5. K值一般选择样本数量的平方根

## 5.3. 算法描述

1. 计算已知类别数据集中点与当前点之间的距离
2. 按照距离增次序排序
3. 选取与当前点距离最小的k个点
4. 统计前k个点所在的类别出现的频率
5. 返回前k个点出现频率最高的类别作为当前点的预测分类
   1. 投票法:可以选择K个点出现频率最高的类别作为预测结果
   2. 平均法:可以计算K个点的实值输出标记的平均值作为预测结果
   3. 加权平均法:根据距离远近完成加权平均等方法

## 5.4. 算法优点

1. 简单有效
2. 重新训练代价低
3. 算法复杂度低
4. 适合类域交叉样本
5. 适用大样本自动分类

## 5.5. 算法缺点

1. 惰性学习
2. 类别分类不标准化
3. 输出可解释性不强
4. 不均衡性
5. 计算量较大

## 5.6. KNN算法的Sklearn实现

> Sklearn KNN声明

```py
def KNeighborsClassifier(n_neighbors = 5,
                       weights='uniform',
                       algorithm = '',
                       leaf_size = '30',
                       p = 2,
                       metric = 'minkowski',
                       metric_params = None,
                       n_jobs = None
                       )
```

1. n_neighbors：KNN 中的"K"，一般默认值为5。
   1. K值较小，就相当于用较小的领域中的训练实例进行预测，训练误差近似误差小(偏差小)，泛化误差会增大(方差大)，换句话说，K值较小就意味着整体模型变得复杂，容易发生过拟合；
   2. K值较大，就相当于用较大领域中的训练实例进行预测，泛化误差小(方差小)，但缺点是近似误差大(偏差大)，换句话说，K值较大就意味着整体模型变得简单，容易发生欠拟合；一个极端是K等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。
2. weights(权重)：最普遍的 KNN 算法无论距离如何，权重都一样，但有时候我们想搞点特殊化，比如距离更近的点让它更加重要。这时候就需要 weight 这个参数了，这个参数有三个可选参数的值，决定了如何分配权重。参数选项如下
   1. uniform：不管远近权重都一样，就是最普通的 KNN 算法的形式。
   2. distance：权重和距离成反比，距离预测目标越近具有越高的权重。
   3. 自定义函数：自定义一个函数，根据输入的坐标值返回对应的权重，达到自定义权重的目的。
3. leaf_size：这个值控制了使用kd树或者球树时，停止建子树的叶子节点数量的阈值。
   1. 值越小，生成的kd树和球树越大，层数越深，建树时间越长。随着样本的数量增加，这个值也在增加，但是过大可能会过拟合，需要通过交叉检验来选择。
   2. 默认值为30
4. algorithm：在 sklearn 中，要构建 KNN 模型有三种构建方式，而当 KD 树也比较慢的时候，则可以试试球树来构建 KNN。参数选项如下：
   1. brute:蛮力实现直接计算距离比较，适用于小数据集
   2. kd_tree:使用 KD 树构建 KNN 模型，适用于比较大数据集
   3. ball_tree:使用球树实现 KNN，适用于KD树解决起来更复杂
   4. auto:默认参数，自动选择合适的方法构建模型.不过当数据较小或比较稀疏时，无论选择哪个最后都会使用 'brute'
5. p：和metric结合使用的，当metric参数是"minkowski"的时候，p=1为曼哈顿距离， p=2为欧式距离。默认为p=2。
6. metric：指定距离度量方法，一般都是使用欧式距离。
   1. euclidean：欧式距离，p=2
   2. manhattan：曼哈顿距离，p=1
   3. chebyshev：切比雪夫距离，p=\infty，D(x, y) = \max|x_i - y_i|(i = 1, 2, ..., n)
   4. minkowski：闵可夫斯基距离，默认参数，\sqrt[q]{\sum\limits_{i=1}\limits^n(|x_i-y_i|)^p}
7. n_jobs：指定多少个CPU进行运算，默认是-1，也就是全部都算。
8. radius：限定半径，默认为1，半径的选择与样本分布有关，可以通过交叉检验来选择一个比较小的半径
9. outlier_labe:int类型，主要用于预测时，如果目标点半径内没有任何训练集的样本点时，应该标记的类别，不建议选择默认值 None,因为这样遇到异常点会报错。一般设置为训练集里最多样本的类别。

k-mean算法(启发式算法)

1. 假设文档是一个实数向量
2. 基于类中点的centroid(也称为重心或均值)的类

$$
\mu(c) = \frac{1}{|c|}\sum\limits_{x \in c}x
$$

3. 将实例重新分配给类是基于到当前类centroid的距离：或者可以用相似性来等同地表述

# 欧式空间的k-mean算法描述

1. 假设是欧几里得空间/距离，k-mean算法过程
   1. 首先确定k(簇数)
   2. 随机选择k个点作为k个聚类的中心来初始化聚类，保证每一点都与其他的点尽可能的远
   3. 填充每个聚类
      1. 对于每个点，将其放置在当前centroid最近的聚类中
      2. 分配所有点后，更新k个聚类的centroid的位置
      3. 将所有点重新分配到它们最近的centroid:有时在群集之间移动点
      4. 重复2和3直到收敛，收敛被定义为点在簇之间不移动，centroid稳定
2. 终止条件:(有很多的候选条件)
   1. 一个固定数量的循环次数
   2. 文档向量分簇不变
   3. centroid的位置不变

## 6.2. k-mean算法示例

### 6.2.1. 例子一

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/3.png)

### 6.2.2. 例子二

| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/4.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/5.png) |
| ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/6.png) |                                                                                           |

## 6.3. 如何正确的选择k值

1. **尝试不同的k**，观察随着k增大，到centroid的平均距离的变化
2. 平均值迅速下降直到右k，然后变化很小

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/7.png)

| K值选择过少                                                                                    | K值选择刚好                                                                                    | K值选择过多                                                                                     |
| ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/8.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/9.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/10.png) |

- k值选择过少:距离centroid长度过大
- k值选择过多:在平均距离上很难有提升

# 6.4. k-mean的优缺点

### 6.4.1. 优点

1. **相对高效**: O(tkn)，n是物体数量，k是聚类数量，t是迭代次数，一般来讲:k,t << n
2. 通常以局部最优值终止。可以使用诸如**模拟退火和遗传算法**之类的技术找到全局最优值

### 6.4.2. 缺点

1. 仅在定义均值时适用
2. 需要预先指定k，即簇数
3. 嘈杂的数据和异常值的问题
4. 不适合发现具有**非凸形状**的聚类

# 硬聚类＆软聚类

硬聚类和软聚类

1. 硬聚类：每个文档恰好属于一个聚类(更简单，更容易做到)
2. 软聚类：一个文档可以属于多个聚类
   1. 对于创建可浏览层次结构的应用程序更有意义
   2. 您可能希望将一双运动鞋分为两类：
      1. 运动服装
      2. 鞋子
   3. 您只能使用软聚类方法来做到这一点。

# 层次聚类

基于层次的核心操作

重复合并(或拆分)距离最近的两个簇

## 7.3. 度量簇距离的变量

1. 单链接:大多数余弦相似度(单链接)的相似度
2. 全链接:最远点的相似度，最小余弦相似度
3. 质心:centroid最接近余弦的聚类
4. 平均链接:成对元素之间的平均余弦

基于层次的方法的三个重要问题

> 分为欧式空间和非欧式空间进行讨论:在非欧式空间中，我们唯一可以谈论的"位置"就是数据点本身，没有两点的"平均值"

1. 您如何表示一个多点的簇？
2. 您如何确定簇的附近？
3. 什么时候停止合并簇？

### 7.4.1. 您如何表示一个多点的簇？

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/13.png)

1. 欧式空间:
   - centroid = 数据点的均值
   - 我们选择使用质心centroid作为簇的代表
2. 非欧式空间:
   - clustroid = (数据)点与其他点"最近"
   - 最近的含义:
     - 最小化每个点之间的最大距离
     - 最小化每个点之间的平均距离
     - 最小化和其他店之间的距离平方和:对于距离矩阵d，簇C的中心c是:\min\limits_c\sum\limits_{x \in C} d(x, c)^2

### 7.4.2. 如何确定簇的附近

1. 欧式空间:
   - 我们使用centroid的距离来度量簇的距离
2. 非欧式空间
   - 当计算类间距离时，我们把clustroid作为centroid
   - clustroid的计算方法
     - 簇内距离:任何两个点之间的最小距离，每个群集中的一个
     - 选择簇的"内聚"概念，例如距clusteroid的最大距离，合并两个簇如果这两个簇的并集是最内聚的。
   - 内聚的定义:
     - 方法一:使用合并簇的直径=簇中点之间的最大距离
     - 方法二:使用簇中点之间的平均距离
     - 方法三:使用基于密度的方法，取直径或平均值。 距离，例如，除以群集中的点数

### 7.4.3. 欧式空间的合并层次方法实例

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec7/12.png)

## 7.5. 基于层次的聚类的实现

1. 在每个步骤中，计算所有簇对之间的成对距离，然后合并
2. 时间复杂度:O(N^3)
3. 如果我们使用优先级队列可以将时间复杂度降解到O(N^2 \log N)，但是对于无法容纳在内存的大型数据集来说仍然太昂贵

## 其他层次聚类方法

1. 聚集聚类方法的主要缺点
   1. 伸缩性不好：时间复杂度至少为O(n^2)，其中n是对象总数
   2. 永远不能撤销以前所做的事情
2. 将层次结构与基于距离的聚类集成
   1. BIRCH：使用CF树并逐步调整子簇的质量
   2. CURE：从聚类中选择分散良好的点，然后将它们向聚类中心缩小指定比例

# 启发式算法

逃离局部最优，全局最优

# 基于协同的推荐系统

1. 协同过滤:利用其他用户的数据来为用户完成推荐

## 3.1. 协同过滤过程

1. 我们考虑用户X
2. 查找评级与用户X评级"相似"的N个其他用户的集合
3. 根据N个用户的评分来估算X的评分，并决定是否推荐给用户X

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/6.png)

## 3.2. 协同过滤(Collaborative Filtering, CF)

1. 协同过滤是推荐的最杰出方法
   1. 由大型商业电子商务网站使用
   2. 很好理解，存在各种算法和变体
   3. 适用于许多领域(书籍，电影，DVD等)
2. 方法：使用"人群的智慧"推荐物品基本假设和想法
3. 假设：在用户通过隐式或显式的方式完成对目录项进行评分，并且过去口味相似的客户在未来也有大可能口味相似

## 3.3. 基于用户的最近邻协同过滤

1. 基本技巧：给定活跃用户X和用户X尚未看到的商品i
   1. 找到一组过去喜欢与X相同的商品并且对商品i进行了评分的用户Y(同龄人/最近的邻居)
   2. 使用推荐系统进行预测，例如预测用户X给商品i的评分会是多少
   3. 对用户X没看过的所有商品执行此操作，并给用户X推荐评分最高的商品
2. 基本假设和想法：如果用户过去具有相似的口味，将来他们也会具有相似的口味。用户偏好会随着时间的推移保持稳定和一致

### 3.3.1. 寻找相似的用户

1. 我们让r_x作为用户x的评分向量
2. Jaccard相似度
   1. J(A,B) = \frac{|A\cap B|}{|A \cup B|}
   2. 问题是忽略了评分的价值
   3. 我们把r_x和r_y作为集合
      1. r_x = \{1, 4, 5\}
      2. r_y = \{1, 3, 4\}
3. 余弦相似度:问题是把没有评分的部分作为负面情况
   1. sim(x,y) = cos(r_x, r_y) = \frac{r_x * r_y}{||r_x||*||r_y||}
   2. 我们把r_x和r_y作为点
      1. r_x = \{1, 0, 0, 1, 3\}
      2. r_y = \{1, 0, 2, 2, 0\}
4. 皮尔逊相关系数，S_{xy}指的是物品和用户x、y的相似性：\overline{r_x}和\overline{r_y}是x和y的均值评分

$$
sim(x,y) = \frac{\sum\limits_{s \in S_{xy}}(r_{xs} - \overline{r_x})(r_{ys} - \overline{r_y})}{\sqrt{\sum\limits_{s \in S_{xy}}(r_{xs} - \overline{r_x})^2 }\sqrt{\sum\limits_{s \in S_{xy}}(r_{ys} - \overline{r_y})^2}}
$$

### 3.3.2. 相似矩阵例子

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/7.png)

> 直观地我们想要说明:Sim(A, B) > Sim(A, C)

1. Jaccard相似度：Sim(A, B) = \frac{1}{5} < Sim(A, C) = \frac{2}{4}
2. 余弦相似度：Sim(A, B) = 0.380 > Sim(A, C) = 0.322
   1. 认为缺失的评分为0，问题:默认是负面评论
      1. A = {4, 0, 0, 5, 1, 0, 0}
      2. B = {5, 5, 4, 0, 0, 0, 0}
      3. C = {0, 0, 0, 2 ,4 ,5, 0}
      4. Sim(A, B) = 0.380 > Sim(A, C) = 0.322
   2. 解决默认负面评论：减去(行)平均值，Sim(A, B) = 0.092 > Sim(A, C) = -0.559

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/8.png)

## 3.4. 评分预测

1. 从相似性指标到推荐：
   1. 设r_x为用户x评分的向量
   2. 设N为与x最相似的，对项目i评分的k个用户的集合
   3. 对用户x的项目s的预测：(S_{xy}是sim(x,y)的缩写)
2. 以下为几种分数预测策略：

$$
\begin{array}{l}
   r_{xi} = \frac{1}{k}\sum\limits_{y\in N} r_{yi} \\
   \ \\
   r_{xi} = \frac{\sum\limits_{y\in N}s_{xy} * r_{yi}}{\sum\limits_{y\in N} s_{xy}}
\end{array}
$$

## 3.5. 物品-物品协同过滤

1. 到目前为止：我们完成了用户-用户的协作过滤
2. 但是我们还有另一种角度：物品
   1. 对于物品i，找到其他类似的物品
   2. 根据类似物品的评级估算物品i的评级
   3. 可以使用与用户-用户模型相同的相似性指标和预测功能

$$
r_{xi} = \frac{\sum\limits_{j \in N(i; x)} s_{ij} * r_{xj}}{\sum\limits_{y \in N(i; x)}S_{ij}}
$$

1. s_{ij}是物品i和j之间的相似度
2. r_{xj}是用户x对物品j的评分
3. N(i;x)是设置由用户x评分与i相似的项目

### 3.5.1. 物品-物品协同过滤的例子

| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/9.png)  | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/10.png) |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/11.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/12.png) |
| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/13.png) |                                                                                            |

### 3.5.2. 更加普遍的协同过滤

- 我们定义项i和j的相似度为s_{ij}
- 选择k个最近的邻居N(i; x):与用户x最相似的项目，由用户x评分
- 将等级r_{xi}估计为加权平均值：

$$
\begin{array}{l}
   Before:r_{xi} = \frac{\sum\limits_{y\in N}s_{xy} * r_{yi}}{\sum\limits_{y\in N} s_{xy}} \\
   \ \\
   After: r_{xi} = b_{xi} + \frac{\sum\limits_{j \in N(i; x)} s_{ij} * (r_{xj} - b_{xj})}{\sum\limits_{j \in N(i; x)}s_{ij}} \\
   \ \\
   b_{xi} = \mu + b_x + b_i
\end{array}
$$

- \mu:电影整体平均收视率
- b_x:用户x的评分偏差 = (用户x的平均评分)- \mu
- b_i:电影i的评分偏差

## 3.6. 对于物品-物品的协同过滤和用户-用户的协同过滤

> 在实际生活中，我们可以观察到物品-物品通常比用户-用户更好地工作
> 因为物品更简单，而用户有很多的口味

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/14.png)

## 3.7. 数据稀疏性问题

1. 冷启动问题：如何推荐新商品？向新用户推荐什么？
2. 直接的方法
   1. 要求/强迫用户对一组项目进行评分
   2. 在初始阶段使用另一种方法(例如，基于内容的，人口统计学的或只是非个性化的)
   3. 默认投票：为只有两个要比较的用户之一进行评分的项目分配默认值(Breese等，1998)
3. 备择方案
   1. 使用更好的算法(超越最近的邻域方法)
   2. 例：
      1. 在最近邻居方法中，足够相似的邻居的集合可能太小而无法做出好的预测
      2. 假设邻居的"传递性"

# 3.8. 协同过滤的优点

> 适用于任何种类的物品：无需选择功能

1. 隐式收集:不需要明确的用户打分和用户交互
2. 大量数据:使用不同的数据来保护用户隐私
3. 如果我们大规模地使用户数据，那么协同过滤更加有效。
4. 基于内容、知识的推荐方法是很难使用在网络数据上

# 3.9. 协同过滤的缺点

1. 冷启动：系统中需要足够的用户才能找到匹配项
2. 稀疏度：
   1. 用户/评分矩阵稀疏
   2. 很难找到评分相同的用户
3. 最初评分：
   1. 无法推荐以前未评级的项目
   2. 新项目，神秘项目
4. 人气偏见：
   1. 无法向有独特品味的人推荐产品
   2. 倾向于推荐热门商品

# 基于内容的推荐系统

1. 核心思想:向用户推荐他自己之前高度评价相似的商品
2. 例子:
   1. 电影推荐:推荐由相同演员、导演和主演的电影
   2. 网络、博客和新闻:推荐由相似内容的其它网站

## 4.1. 基于内容的推荐系统概述

1. 协同过滤方法不需要有关项目的任何信息
   1. 利用此类信息可能是合理的
   2. 向过去喜欢幻想小说的人推荐幻想小说
2. 我们需要什么：
   1. 有关可用项目的一些信息，例如类型("内容")
   2. 描述用户喜欢的某种用户配置文件(首选项)
3. 任务：
   1. 了解用户偏好
   2. 查找/推荐与用户首选项"相似"的项目

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/35.png)

## 4.2. 推荐过程

> 对于文本信息提取的时候 会使用到TF-IDF算法

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/5.png)

## 4.3. 物品资料

1. 为每个物品创建一个物品资料
2. 物品资料是一组特征(向量)
   1. 电影：作者，标题，演员，导演，…
   2. 文本：文档中的一组"重要"词
3. 如何选择重要功能？以文本挖掘为例，我们通常使用启发式方法为TF-IDF算法
   1. 词：功能
   2. 文档：项目

## 4.4. 用户资料

1. 用户资料的可能形式
   1. 特定物品资料的加权平均值
   2. 差异：权重与平均评分的差异
2. 预测启发式：给定用户资料x和项目资料i，估算余弦相似度为u(x,i) = cos(x,i) = \frac{x*i}{||x|| * ||i||}

# 基于内容方法的优点

1. 无需其他用户的数据：无冷启动或稀疏问题
2. **可以向口味独特的用户推荐**
3. **可以推荐不受欢迎的新商品**
4. 能够提供**解释**：可以通过列出导致推荐项目的内容功能来提供推荐项目的说明
5. 需要设置衰减机制来模拟喜好的变更

# 基于内容方法的缺点

1. 很难找到合适的功能：例如图像，电影，音乐
2. 对新用户的建议：如何建立用户档案？
3. **过度专业化**
   1. 绝不推荐用户资料之外的项目
   2. 人们可能有多种兴趣
   3. 无法利用其他用户的质量判断

## 4.7. 讨论和总结

1. 与基于协同过滤的推荐方法相反，基于内容的推荐方法不需要用户数据即可工作
2. 提出的方法旨在基于**显式或隐式反馈**来学习用户兴趣偏好的模型：从用户行为中获取隐式反馈可能会出现问题
3. 评估表明，借助机器学习技术可以实现良好的推荐准确性：这些技术不需要用户社区
4. 存在推荐列表包含太多相似物品的危险
   1. 所有学习技术都需要一定数量的训练数据
   2. 一些学习方法倾向于过度拟合训练数据
5. 在商业环境中很少发现基于纯内容的系统

# evaluating prediction

评估

| ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/15.png) | ![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec8/16.png) |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |

- 将预测与已知评级进行比较
  - 均方根误差(RMSE):\sqrt{\sum\limits_{xi}(r_{xi} - r_{xi}^*)^2}，其中r_{xi}为预测值，r_{xi}^* 是x在i上的真实评级
  - 预测前10位
  - 排序相关性：Spearman相关性在系统和用户完整排名之间
- 另一种方法:0/1模型
  - 承保范围：系统可以预测的项目/用户数
  - 精度：预测的准确性
  - 接收器工作特性(ROC)：误报与误报之间的折衷曲线

# spark

流运算流程

# 为什么会选择Spark(重要)

1. Apache Spark是一个开放源代码集群计算框架，最初是在加利福尼亚大学伯克利分校的AMPLab中开发的，但后来捐赠给了Apache软件基金会，至今仍在使用。与Hadoop的基于磁盘的分析范例相反，Spark具有多阶段内存中分析。

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec12/27.png)

1. 速度快：相同的任务、硬件设备上，在内存中，Spark要比Hadoop快100倍，在硬盘中，Spark要比Hadoop快10倍左右。因为是基于内存的，Spark提出了数据内存抽象，RDD允许用户在查询过程中将集合存放在内存中。
2. 使用方便：可以很快的使用Java、Scala、Python和R进行开发。
3. 通用性：集合了SQL、Streaming和复杂的分析。

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec12/28.png)

> RDD：resilient distributed datasets，弹性分布式数据集

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec12/29.png)

1. RDD(弹性分布式数据集)是Spark中的主要逻辑数据单元。RDD是对象的**分布式集合**。分布式方式，每个RDD分为多个**分区**。这些分区中的每个分区都可以驻留在内存中或存储在群集中不同计算机的磁盘上。RDD是**不可变的(只读)数据结构**。您无法更改原始RDD，但可以随时将其更改为具有所有所需更改的RDD。
2. 在Spark中，所有工作都表示为
   1. 创建新的RDD
   2. 转换现有的RDD
   3. 对RDD调用操作以计算结果
3. 转换按需执行。这意味着它们是惰性计算的。 例如：过滤，加入，排序
4. 动作将返回RDD计算的最终结果。 Actions使用沿袭图触发执行，以将数据加载到原始RDD中，执行所有中间转换并将最终结果返回到Driver程序或将其写到文件系统中。 例如：collect()，count()，take()

## Spark工作流

- Spark应用程序在群集上作为独立的进程集运行，由主程序(称为驱动程序)中的SparkContext对象协调。
- 要求集群管理器在各个应用程序之间分配资源。
- 连接后，Spark会在集群中的节点上获取执行程序，这些节点是运行计算并为您的应用程序存储数据的进程。
- 接下来，它将您的应用程序代码(由传递给SparkContext的JAR或Python文件定义)发送给执行者。最后，SparkContext将任务发送给执行程序以运行。

## 功能

1. Spark SQL
   1. 前身是Shark，基于Hive的Spark SQL，代码量大、复杂，难优化和维护
   2. 交互式查询、标准访问接口、兼容Hive
   3. 专门用于处理结构化数据：分布式SQL引擎；在Spark 程序中调用API
2. Spark Streaming：实时对**大量数据进行快速处理**，处理周期短
3. Spark GraphX：**以图为基础**数据结构的算法实现和相关应用
4. Spark MLlib：为解决**机器学习**开发的库，包括分类、回归、聚类和协同过滤等

## 集群管理框架

1. Standalone mode：独立集群管理功能：任务调度、资源分配等
2. Apache Mesos：从分布式计算节点上抽象CPU, memory, storage, and other compute resources给其他框架使用，实现了静态资源分配功能
3. Hadoop Yarn：Hadoop MapReduce的第二个版本架构，把资源管理和任务管理剥离开；实现了静态资源分配和动态资源分配功能；
4. EC2：Amazon EC2云平台，提供一个安装了Spark、Shark 和HDFS的集群，可直接登录到集群，把它当作你实验室的集群使用

## Spark Streaming

1. 实时对大量数据进行快速处理：处理周期短；连续不断地计算
2. 计算基本过程

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec12/43.png)

3. 数据分批

![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2020-Big-data-analysis/img/lec12/44.png)

4. StreamingContext (类似SparkContext)
5. Dstream (类似RDD)
   1. 内部是通过RDD实现的
   2. 每个时间点对应一个RDD
6. 程序执行过程
   1. 初始化StreamingContext
   2. 创建Dstream
   3. 对DStream的操作
      1. Transformation操作
      2. 参考RDD
      3. transform操作：直接操作内部RDD
      4. 窗口(Window)操作：合并几个时间点的RDD
      5. Output操作
      6. print()
      7. saveAsTextFiles/saveAsObjectFiles()
      8. foreachRDD(func): func一般是对RDD的Action操作
